{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import hashlib\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_png(directory, label, target):\n",
    "    for i, image in enumerate(os.listdir(directory)):\n",
    "        f = os.path.join(directory, image)\n",
    "        if os.path.isfile(f):\n",
    "            image = Image.open(f)\n",
    "            image.save(f'data/{i}{label}.{target}')\n",
    "            \n",
    "    os.system('rmdir /S /Q \"{}\"'.format(directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    DO NOT RUN\n",
    "\"\"\"\n",
    "\n",
    "directory_1 = 'data/cancer'\n",
    "directory_2 = 'data/non-cancer'\n",
    "target = 'png'\n",
    "\n",
    "convert_png(directory_1, 1, target)\n",
    "convert_png(directory_2, 0, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePreprocessor:\n",
    "    \"\"\"\n",
    "    A class for preprocessing images.\n",
    "\n",
    "    Args:\n",
    "        image_dir (str): The directory containing the images.\n",
    "        target_size (tuple): The target size of the images after resizing.\n",
    "        normalize_range (tuple): The range to normalize the pixel values to.\n",
    "\n",
    "    Attributes:\n",
    "        image_dir (str): The directory containing the images.\n",
    "        target_size (tuple): The target size of the images after resizing.\n",
    "        normalize_range (tuple): The range to normalize the pixel values to.\n",
    "        image_hashes (dict): A dictionary to store the image hashes.\n",
    "\n",
    "    Methods:\n",
    "        hash_image(image): Computes the hash value of an image.\n",
    "        check_duplicates(): Checks for duplicate images and performs augmentation if necessary.\n",
    "        augment_image(image): Applies a random augmentation to an image.\n",
    "        resize_image(image): Resizes an image to the target size.\n",
    "        normalize_image(image): Normalizes the pixel values of an image.\n",
    "        convert_to_tensor(image): Converts an image to a tensor.\n",
    "        preprocess_image(): Preprocesses the images in the image directory.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_dir, target_size, normalize_range):\n",
    "            \"\"\"\n",
    "            Initializes a SmileSavior object.\n",
    "\n",
    "            Args:\n",
    "                image_dir (str): The directory path where the images are stored.\n",
    "                target_size (tuple): The desired size of the images after resizing.\n",
    "                normalize_range (bool): Flag indicating whether to normalize the pixel values of the images.\n",
    "\n",
    "            Attributes:\n",
    "                image_dir (str): The directory path where the images are stored.\n",
    "                target_size (tuple): The desired size of the images after resizing.\n",
    "                normalize_range (bool): Flag indicating whether to normalize the pixel values of the images.\n",
    "                image_hashes (dict): A dictionary to store the hashes of the images.\n",
    "\n",
    "            \"\"\"\n",
    "            self.image_dir = image_dir\n",
    "            self.target_size = target_size\n",
    "            self.normalize_range = normalize_range\n",
    "            self.image_hashes = {}\n",
    "\n",
    "    def hash_image(self, image):\n",
    "        \"\"\"\n",
    "        Calculates the MD5 hash of the given image.\n",
    "\n",
    "        Parameters:\n",
    "        image (numpy.ndarray): The image to be hashed.\n",
    "\n",
    "        Returns:\n",
    "        str: The MD5 hash of the image.\n",
    "        \"\"\"\n",
    "        return hashlib.md5(image.tobytes()).hexdigest()\n",
    "\n",
    "    def check_duplicates(self):\n",
    "        \"\"\"\n",
    "        Check for duplicate images in the specified image directory and perform image augmentation if duplicates are found.\n",
    "\n",
    "        This method iterates over all the images in the specified image directory and checks if each image has a duplicate.\n",
    "        If a duplicate is found, the image is augmented and saved, replacing the original image. If no duplicate is found,\n",
    "        the image is added to the list of image hashes for future comparison.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        images = glob.glob(os.path.join(self.image_dir, '*'))\n",
    "        for image_path in images:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image_hash = self.hash_image(image)\n",
    "            \n",
    "            if image_hash in self.image_hashes:\n",
    "                image = self.augment_image(image)\n",
    "                image.save(image_path)\n",
    "            else:\n",
    "                self.image_hashes[image_hash] = image_path\n",
    "\n",
    "    def augment_image(self, image):\n",
    "            \"\"\"\n",
    "            Apply a random augmentation to the given image.\n",
    "\n",
    "            Parameters:\n",
    "            image (PIL.Image.Image): The input image to be augmented.\n",
    "\n",
    "            Returns:\n",
    "            PIL.Image.Image: The augmented image.\n",
    "            \"\"\"\n",
    "            augmentations = [\n",
    "                ImageOps.mirror,\n",
    "                ImageOps.flip,\n",
    "                lambda img: img.rotate(90)\n",
    "            ]\n",
    "            augmentation = np.random.choice(augmentations)\n",
    "            return augmentation(image)\n",
    "    \n",
    "    def resize_image(self, image):\n",
    "        \"\"\"\n",
    "        Resizes the given image to the target size.\n",
    "\n",
    "        Parameters:\n",
    "        - image: The image to be resized.\n",
    "\n",
    "        Returns:\n",
    "        - The resized image.\n",
    "        \"\"\"\n",
    "        return image.resize(self.target_size)\n",
    "\n",
    "    def normalize_image(self, image):\n",
    "        \"\"\"\n",
    "        Normalize the given image.\n",
    "\n",
    "        Args:\n",
    "            image (PIL.Image.Image): The input image to be normalized.\n",
    "\n",
    "        Returns:\n",
    "            PIL.Image.Image: The normalized image.\n",
    "\n",
    "        \"\"\"\n",
    "        image_array = np.array(image).astype(np.float32)\n",
    "        image_array /= 255.0\n",
    "        if self.normalize_range == (-1, 1):\n",
    "            image_array = image_array * 2 - 1\n",
    "        return Image.fromarray((image_array * 255).astype(np.uint8))\n",
    "    \n",
    "    def convert_to_tensor(self, image):\n",
    "        \"\"\"\n",
    "        Converts an image to a PyTorch tensor.\n",
    "\n",
    "        Args:\n",
    "            image (PIL.Image.Image): The input image.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The converted tensor representation of the image.\n",
    "        \"\"\"\n",
    "        transform = transforms.ToTensor()\n",
    "        return transform(image)\n",
    "    \n",
    "    def preprocess_image(self):\n",
    "            \"\"\"\n",
    "            Preprocesses the images in the specified directory.\n",
    "\n",
    "            Returns:\n",
    "            processed_images (list): A list of processed images.\n",
    "            \"\"\"\n",
    "            self.check_duplicates()\n",
    "            images = glob.glob(os.path.join(self.image_dir, '*'))\n",
    "            if not images:\n",
    "                print(\"No images found in the directory.\")\n",
    "\n",
    "            processed_images = []\n",
    "            labels = []\n",
    "            for image_path in images:\n",
    "                print(f\"Processing image: {image_path}\")\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                image = self.resize_image(image)\n",
    "                print(f\"Resized image: {image.size}\")\n",
    "                image = self.normalize_image(image)\n",
    "                print(f\"Normalized image: {np.array(image).shape}\")\n",
    "                image = self.convert_to_tensor(image)\n",
    "                print(f\"Converted to tensor: {image.shape}\")\n",
    "                processed_images.append(image)\n",
    "                \n",
    "                target = image_path.split('.')[0][-1]\n",
    "                labels.append(target)\n",
    "                \n",
    "            return torch.stack(processed_images), labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Glen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\PIL\\Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image: data\\00.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\01.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\10.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\100.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\101.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\11.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\110.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\111.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\120.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\121.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\130.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\131.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\140.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\141.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\150.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\151.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\160.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\161.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\170.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\171.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\180.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\181.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\190.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\191.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\20.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\200.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\201.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\21.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\210.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\211.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\220.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\221.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\230.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\231.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\240.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\241.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\250.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\251.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\260.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\261.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\270.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\271.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\280.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\281.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\290.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\291.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\30.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\300.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\301.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\31.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\310.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\311.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\320.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\321.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\330.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\331.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\340.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\341.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\350.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\351.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\360.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\361.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\370.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\371.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\380.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\381.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\390.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\391.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\40.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\400.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\401.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\41.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\410.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\411.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\420.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\421.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\430.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\431.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\441.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\451.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\461.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\471.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\481.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\491.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\50.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\501.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\51.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\511.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\521.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\531.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\541.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\551.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\561.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\571.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\581.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\591.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\60.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\601.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\61.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\611.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\621.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\631.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\641.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\651.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\661.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\671.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\681.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\691.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\70.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\701.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\71.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\711.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\721.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\731.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\741.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\751.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\761.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\771.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\781.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\791.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\80.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\801.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\81.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\811.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\821.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\831.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\841.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\851.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\861.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\90.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "Processing image: data\\91.png\n",
      "Resized image: (224, 224)\n",
      "Normalized image: (224, 224, 3)\n",
      "Converted to tensor: torch.Size([3, 224, 224])\n",
      "-------------------------------------------------------------Completed Preprocessing-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "preprocessor = ImagePreprocessor('data', (224, 224), (0, 1))\n",
    "data, labels = preprocessor.preprocess_image()\n",
    "print(\"-------------------------------------------------------------Completed Preprocessing-------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([131, 3, 224, 224]), 131)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape, len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Glen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Glen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Flatten(start_dim=1, end_dim=-1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class PrototypicalNetwork(nn.Module):\n",
    "    def __init__(self, encoding_function):\n",
    "        super(PrototypicalNetwork, self).__init__()\n",
    "        self.encoding_function = encoding_function\n",
    "        \n",
    "    def forward(self, support_set, support_labels, query_set):\n",
    "        # Getting embeddings for support and query set by forward propagating through pre-trained RestNet model\n",
    "        support_set = self.encoding_function.forward(support_set)\n",
    "        query_set = self.encoding_function.forward(query_set)\n",
    "        \n",
    "        # No. of classes\n",
    "        n_way = 2\n",
    "        \n",
    "        # Computing class prototypes (mean of all instances to their corresponding label)\n",
    "        class_prototypes = torch.cat(\n",
    "            [\n",
    "                support_set[torch.nonzero(support_labels == label)].mean(0)\n",
    "                for label in range(n_way)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Computing the euclidian distance between queries and class prototypes\n",
    "        distances = torch.cdist(query_set, class_prototypes)\n",
    "        \n",
    "        # Using softmax to transform distances to classification scores\n",
    "        softmax = nn.Softmax(dim=distances.dim)\n",
    "        classification_scores = softmax(distances)\n",
    "        \n",
    "        return classification_scores\n",
    "\n",
    "encoding_function = resnet50(pretrained=True)\n",
    "encoding_function.fc = nn.Flatten()\n",
    "print(encoding_function)\n",
    "\n",
    "PrototypicalNetwork_model = PrototypicalNetwork(encoding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.BCELoss()\n",
    "optimizer = optim.SGD(encoding_function.parameters(), lr=0.01)\n",
    "\n",
    "def train(support_data, support_labels, query_data, query_labels):\n",
    "    # Intializes gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    classification_scores = encoding_function(support_data, support_labels, query_data)\n",
    "\n",
    "    loss_ = loss(classification_scores, query_labels.cuda())\n",
    "    loss_.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
